{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107da808",
   "metadata": {},
   "source": [
    "## Automated Sales Outreach Agent\n",
    "\n",
    "Welcome to your Automated Sales Outreach project!\n",
    "\n",
    "We're going to build a comprehensive Agent system for generating and sending cold sales outreach emails using Gemini and Brevo:\n",
    "1. **Agent workflow**: Parallel generation of email drafts.\n",
    "2. **Selection**: An agent to pick the best draft.\n",
    "3. **Tools**: Integrating Brevo (Sendinblue) to actually send emails.\n",
    "4. **Handoffs**: Specialist agents for subject lines and HTML formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debca9d5",
   "metadata": {},
   "source": [
    "## Before we start - Setup:\n",
    "\n",
    "Ensure you have a **Brevo** account (formerly Sendinblue) at: https://www.brevo.com/\n",
    "\n",
    "1.  **API Key**: Go to your Profile > SMTP & API > Generate a new API Key. Add this to your `.env` file as `BREVO_API_KEY`.\n",
    "2.  **Sender Verification**: Ensure you have a verified sender email in Brevo.\n",
    "3.  **Gemini API Key**: Ensure you have `GEMINI_API_KEY` (or `GOOGLE_API_KEY`) in your `.env` for the AI model.\n",
    "\n",
    "We will load these environments variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e15e6487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:09:36.665722Z",
     "iopub.status.busy": "2025-12-18T14:09:36.665722Z",
     "iopub.status.idle": "2025-12-18T14:09:38.439942Z",
     "shell.execute_reply": "2025-12-18T14:09:38.439054Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import sib_api_v3_sdk\n",
    "from sib_api_v3_sdk.rest import ApiException\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace, function_tool\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "# 1. Force reload environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 2. Verify Keys exist before running\n",
    "if not os.getenv(\"BREVO_API_KEY\"):\n",
    "    raise ValueError(\"Missing BREVO_API_KEY in .env file\")\n",
    "\n",
    "# Handle GOOGLE_API_KEY / GEMINI_API_KEY mismatch fallback\n",
    "if not os.getenv(\"GEMINI_API_KEY\") and os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GEMINI_API_KEY\"] = os.environ[\"GOOGLE_API_KEY\"]\n",
    "\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    raise ValueError(\"Missing GEMINI_API_KEY (or GOOGLE_API_KEY) in .env file\")\n",
    "\n",
    "# The API key will be automatically picked up by LiteLLM / Agent SDK\n",
    "OPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e266d680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:09:38.442700Z",
     "iopub.status.busy": "2025-12-18T14:09:38.442700Z",
     "iopub.status.idle": "2025-12-18T14:09:38.447365Z",
     "shell.execute_reply": "2025-12-18T14:09:38.446657Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# UPDATE THIS to your verified Brevo sender\n",
    "SENDER_EMAIL = {\"email\": \"abelmarie49@gmail.com\", \"name\": \"Abel Marie\"} \n",
    "RECIPIENT_EMAIL = [{\"email\": \"birukabere4@gmail.com\", \"name\": \"Dick Head\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c0c7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE MODELS (AUTO FALLBACK)\n",
    "# -------------------------------------------------\n",
    "FREE_MODELS = [\n",
    "    \"litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\",\n",
    "    \"litellm/openrouter/mistralai/mistral-7b-instruct:free\",\n",
    "    \"litellm/openrouter/tngtech/deepseek-r1t2-chimera:free\",\n",
    "    \"litellm/openrouter/meta-llama/llama-3.3-70b-instruct:free\",\n",
    "    \"litellm/openrouter/nousresearch/hermes-3-llama-3.1-405b:free\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5cb03dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:09:38.450540Z",
     "iopub.status.busy": "2025-12-18T14:09:38.449256Z",
     "iopub.status.idle": "2025-12-18T14:09:38.461456Z",
     "shell.execute_reply": "2025-12-18T14:09:38.459975Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Helper: Brevo Configuration ---\n",
    "def get_brevo_api_instance():\n",
    "    configuration = sib_api_v3_sdk.Configuration()\n",
    "    configuration.api_key['api-key'] = os.environ.get('BREVO_API_KEY')\n",
    "    return sib_api_v3_sdk.TransactionalEmailsApi(sib_api_v3_sdk.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6d036",
   "metadata": {},
   "source": [
    "## Step 1: Agent Workflow\n",
    "\n",
    "We will define three distinct personalities for our sales agents to generate different styles of copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27359b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions1 = \"You are a sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write professional, serious cold emails.\"\n",
    "\n",
    "instructions2 = \"You are a humorous, engaging sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write witty, engaging cold emails that are likely to get a response.\"\n",
    "\n",
    "instructions3 = \"You are a busy sales agent working for ComplAI, \\\n",
    "a company that provides a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI. \\\n",
    "You write concise, to the point cold emails.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab85b6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:09:38.464612Z",
     "iopub.status.busy": "2025-12-18T14:09:38.464085Z",
     "iopub.status.idle": "2025-12-18T14:09:38.469323Z",
     "shell.execute_reply": "2025-12-18T14:09:38.469323Z"
    }
   },
   "outputs": [],
   "source": [
    "AGENT_DEFINITIONS = [\n",
    "    (\"Professional Sales Agent\", instructions1),\n",
    "    (\"Engaging Sales Agent\", instructions2),\n",
    "    (\"Busy Sales Agent\", instructions3),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd7d7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short cold sales email\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470cdc97",
   "metadata": {},
   "source": [
    "# Sequental Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agents():\n",
    "    for agent_name, instructions in AGENT_DEFINITIONS:\n",
    "        print(f\"\\n--- {agent_name} ---\\n\")\n",
    "\n",
    "        last_error = None\n",
    "\n",
    "        for model in FREE_MODELS:\n",
    "            print(f\"üîÅ Trying model ‚Üí {model}\")\n",
    "\n",
    "            agent = Agent(\n",
    "                name=agent_name,\n",
    "                instructions=instructions,\n",
    "                model=model\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                result = Runner.run_streamed(agent, input=prompt)\n",
    "\n",
    "                async for event in result.stream_events():\n",
    "                    if event.type == \"raw_response_event\":\n",
    "                        if hasattr(event.data, \"delta\") and event.data.delta:\n",
    "                            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "                print(\"\\n‚úÖ Success\\n\")\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                print(f\"‚ö†Ô∏è Model failed: {model}\")\n",
    "                print(f\"Reason: {e}\\n\")\n",
    "                await asyncio.sleep(1)\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå All free models failed.\")\n",
    "            raise last_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9f9504a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Professional Sales Agent ---\n",
      "\n",
      "üîÅ Trying model ‚Üí litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Subject: Simplify Your SOC2 Comp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liance with AI-Powered Solutions\n",
      "\n",
      "Dear [Recipient's Name],\n",
      "\n",
      "As a compliance and risk management professional, you understand the importance of maintaining SOC2 compliance. However, navigating the complex auditing process can be time-consuming and costly.\n",
      "\n",
      "At ComplAI, we offer a cutting-edge SaaS solution that empowers organizations like yours to ensure SOC2 compliance with ease. Our AI-powered tool streamlines the audit preparation process, providing real-time reporting and automated risk assessments.\n",
      "\n",
      "By partnering with ComplAI, you can:\n",
      "\n",
      "- Improve audit readiness with our comprehensive audit preparations\n",
      "- Enhance stakeholder confidence through transparent reporting\n",
      "- Reduce audit costs by up to 30%\n",
      "\n",
      "Discover how ComplAI can simplify your SOC2 compliance journey. Schedule a personalized demo today and experience the future of audit preparedness.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "ComplAI Sales Team\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "‚úÖ Success\n",
      "\n",
      "\n",
      "--- Engaging Sales Agent ---\n",
      "\n",
      "üîÅ Trying model ‚Üí litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a short cold sales email:\n",
      "\n",
      "Subject: Simplify Your Compliance Checks with ComplAI\n",
      "\n",
      "Hi [Name],\n",
      "\n",
      "As the year winds down, many of us are reflecting on the challenges we faced in meeting our compliance goals. Ensuring SOC2 compliance can be a nightmare, but it doesn't have to be.\n",
      "\n",
      "Our team at ComplAI has worked with numerous companies like yours to streamline their compliance processes. With our cutting-edge SaaS tool, you can identify potential risks, stay on top of your compliance posture, and pass audits with flying colors.\n",
      "\n",
      "We'd love to show you how our platform can save you time and headaches in the new year. Would you be open to a quick demo to see how ComplAI can help you simplify your compliance checks?\n",
      "\n",
      "Best,\n",
      "[Your\n",
      " Name\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "]\n",
      "ComplAI Team\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "‚úÖ Success\n",
      "\n",
      "\n",
      "--- Busy Sales Agent ---\n",
      "\n",
      "üîÅ Trying model ‚Üí litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Model failed: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Trying model ‚Üí litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      " <s> \n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "‚úÖ Success\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Jupyter, you can await directly in the cell\n",
    "await run_agents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f612b",
   "metadata": {},
   "source": [
    "### Parallel Generation\n",
    "\n",
    "Now we generate drafts from all three agents in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67a7229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent_with_fallback(agent_name, instructions, message):\n",
    "    last_error = None\n",
    "\n",
    "    for model in FREE_MODELS:\n",
    "        print(f\"üîÅ {agent_name} ‚Üí Trying model: {model}\")\n",
    "\n",
    "        agent = Agent(\n",
    "            name=agent_name,\n",
    "            instructions=instructions,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = await Runner.run(agent, message)\n",
    "            print(f\"‚úÖ {agent_name} succeeded with {model}\\n\")\n",
    "            return result.final_output\n",
    "\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"‚ö†Ô∏è {agent_name} failed on {model}\")\n",
    "            print(f\"Reason: {e}\\n\")\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    print(f\"‚ùå {agent_name}: all free models failed.\")\n",
    "    raise last_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b7522b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Professional Sales Agent ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "üîÅ Engaging Sales Agent ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "üîÅ Busy Sales Agent ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Professional Sales Agent failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "‚ö†Ô∏è Busy Sales Agent failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "‚ö†Ô∏è Engaging Sales Agent failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Professional Sales Agent ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "üîÅ Busy Sales Agent ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "üîÅ Engaging Sales Agent ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Professional Sales Agent succeeded with litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Busy Sales Agent succeeded with litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Engaging Sales Agent succeeded with litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = \"Write a cold sales email\"\n",
    "\n",
    "with trace(\"Parallel cold emails\"):\n",
    "    outputs = await asyncio.gather(\n",
    "        *[\n",
    "            run_agent_with_fallback(agent_name, instructions, message)\n",
    "            for agent_name, instructions in AGENT_DEFINITIONS\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcb053e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to a potential customer who is preparing for a SOC2 audit and is a mid-sized SaaS company. The customer is a CTO who is preparing for their first SOC2 audit.\n",
      "\n",
      "---\n",
      "\n",
      "**Subject:** Simplify Your First SOC2 Audit with ComplAI\n",
      "\n",
      "Hi [First Name],\n",
      "\n",
      "I hope this email finds you well. As a CTO preparing for your first SOC2 audit, I understand the complexity and time investment required to ensure compliance. At ComplAI, we specialize in helping mid-sized SaaS companies like yours streamline the SOC2 audit process with our AI-powered SaaS tool.\n",
      "\n",
      "Our platform automates evidence collection, policy generation, and risk assessments, significantly reducing the manual effort and stress associated with SOC2 compliance. With ComplAI, you can focus on your core business while we handle the compliance heavy lifting.\n",
      "\n",
      "Would you be open to a quick 15-minute call next week to discuss how ComplAI can support your audit preparation? I‚Äôd love to learn more about your current process and share how we‚Äôve helped similar companies save time and resources.\n",
      "\n",
      "Looking forward to your thoughts.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "[Your Position]\n",
      "ComplAI\n",
      "[Your Email]\n",
      "[Your Phone Number]\n",
      "[Company Website]\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      " for a company that offers a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI.\n",
      "\n",
      "---\n",
      "\n",
      "**Subject:** üöÄ SOC 2 Compliance Made Easy (Thanks, AI!)\n",
      "\n",
      "Hi [First Name],\n",
      "\n",
      "Let‚Äôs be real‚ÄîSOC 2 compliance can feel like trying to assemble IKEA furniture blindfolded. But what if I told you there‚Äôs a way to make it *almost* fun? (Okay, maybe not *fun*, but definitely less painful.)\n",
      "\n",
      "At ComplAI, we‚Äôve built an AI-powered SaaS tool that automates the heavy lifting of SOC 2 compliance and audit prep. No more spreadsheets that make you question your life choices. No more last-minute scrambles when the auditor calls. Just a smooth, streamlined process that keeps your security tight and your sanity intact.\n",
      "\n",
      "Here‚Äôs the deal:\n",
      "‚úÖ **AI-driven automation** to track controls, evidence, and gaps‚Äîso you don‚Äôt have to.\n",
      "‚úÖ **Audit-ready reports** in a snap, because who has time for manual documentation?\n",
      "‚úÖ **Real-time insights** to keep you ahead of compliance headaches.\n",
      "\n",
      "Think of us as your SOC 2 sidekick‚Äîhere to save the day (and your inbox).\n",
      "\n",
      "Want to see how it works? Let‚Äôs hop on a quick call or I‚Äôll send you a demo. Either way, you‚Äôll thank yourself later.\n",
      "\n",
      "Cheers,\n",
      "[Your Name]\n",
      "[Your Title]\n",
      "ComplAI\n",
      "[Your Email] | [Your Phone]\n",
      "[Website]\n",
      "\n",
      "P.S. If you‚Äôre already a SOC 2 pro, kudos! But even the best need a little AI magic. Let‚Äôs chat. üòâ\n",
      "\n",
      "---\n",
      "\n",
      "This email keeps it light, highlights the pain points, and makes the value clear‚Äîall while keeping it engaging.\n",
      "\n",
      "\n",
      " for a company that sells a SaaS tool for ensuring SOC2 compliance and preparing for audits, powered by AI.\n",
      "\n",
      "---\n",
      "\n",
      "**Subject:** Streamline Your SOC2 Compliance with AI-Powered Automation\n",
      "\n",
      "Hi [First Name],\n",
      "\n",
      "I came across [Company Name] and was impressed by your focus on [specific aspect of their business, e.g., \"scalable cloud solutions\" or \"data security\"]. As you likely know, SOC2 compliance is a critical step for SaaS companies like yours, but the process can be time-consuming and complex.\n",
      "\n",
      "That‚Äôs where **ComplAI** comes in. Our AI-powered platform automates much of the SOC2 compliance process, from evidence collection to audit preparation, saving your team time and reducing the risk of errors. With ComplAI, you can:\n",
      "\n",
      "- **Automate evidence gathering** ‚Äì No more manual spreadsheets or last-minute scrambles.\n",
      "- **Stay audit-ready year-round** ‚Äì Continuous monitoring keeps you prepared for any audit.\n",
      "- **Reduce compliance costs** ‚Äì Cut down on consulting fees and internal overhead.\n",
      "\n",
      "Many companies like yours have seen a **50% reduction in audit prep time** by using ComplAI. I‚Äôd love to show you how it works‚Äîwould you be open to a quick 15-minute demo?\n",
      "\n",
      "Let me know a time that works for you, or feel free to reply with any questions.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "[Your Position]\n",
      "ComplAI\n",
      "[Your Email] | [Your Phone Number]\n",
      "[Website URL]\n",
      "\n",
      "P.S. If you‚Äôre already using a compliance tool, I‚Äôd still love to hear your thoughts‚Äîreply and let me know what‚Äôs working (or not working) for you!\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    print(output + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be85c9",
   "metadata": {},
   "source": [
    "### Selection Agent\n",
    "\n",
    "We need a \"Sales Picker\" agent to review the drafts and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfabe62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_picker_instructions = (\n",
    "    \"You pick the best cold sales email from the given options. \"\n",
    "    \"Imagine you are a customer and pick the one you are most likely to respond to. \"\n",
    "    \"Do not give an explanation; reply with the selected email only.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e198cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_with_fallback(agent_name, instructions, message):\n",
    "    last_error = None\n",
    "\n",
    "    for model in FREE_MODELS:\n",
    "        print(f\"üîÅ {agent_name} ‚Üí Trying model: {model}\")\n",
    "\n",
    "        agent = Agent(\n",
    "            name=agent_name,\n",
    "            instructions=instructions,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = await Runner.run(agent, message)\n",
    "            print(f\"‚úÖ {agent_name} succeeded with {model}\\n\")\n",
    "            return result.final_output\n",
    "\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"‚ö†Ô∏è {agent_name} failed on {model}\")\n",
    "            print(f\"Reason: {e}\\n\")\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    print(f\"‚ùå {agent_name}: all free models failed.\")\n",
    "    raise last_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6eea72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Professional Sales Agent ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "üîÅ Engaging Sales Agent ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "üîÅ Busy Sales Agent ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Engaging Sales Agent failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Busy Sales Agent failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "‚ö†Ô∏è Professional Sales Agent failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.RateLimitError: RateLimitError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":429,\"metadata\":{\"raw\":\"meta-llama/llama-3.2-3b-instruct:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations\",\"provider_name\":\"Venice\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "üîÅ Engaging Sales Agent ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "üîÅ Professional Sales Agent ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "üîÅ Busy Sales Agent ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Busy Sales Agent succeeded with litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Engaging Sales Agent succeeded with litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Professional Sales Agent succeeded with litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = \"Write a cold sales email\"\n",
    "\n",
    "with trace(\"Selection from sales people\"):\n",
    "    outputs = await asyncio.gather(\n",
    "        run_with_fallback(\"Professional Sales Agent\", instructions1, message),\n",
    "        run_with_fallback(\"Engaging Sales Agent\", instructions2, message),\n",
    "        run_with_fallback(\"Busy Sales Agent\", instructions3, message),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "596e8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = \"Cold sales emails:\\n\\n\" + \"\\n\\nEmail:\\n\\n\".join(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec2be203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Sales Picker ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "‚úÖ Sales Picker succeeded with litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_email = await run_with_fallback(\n",
    "    agent_name=\"Sales Picker\",\n",
    "    instructions=sales_picker_instructions,\n",
    "    message=emails,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b66c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sales email:\n",
      "Subject: Streamline Your SOC2 Compliance with AI-Powered Efficiency\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best sales email:\\n{best_email}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e900eb0",
   "metadata": {},
   "source": [
    "## Step 2: Use of Tools\n",
    "\n",
    "Now we convert our functionality into Tools so the agents can take actions.\n",
    "\n",
    "1. **Email Sending Tool**: Wraps the Brevo API.\n",
    "2. **Agent Tools**: Wraps our personas so a manager can call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d565dfd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:09:44.004253Z",
     "iopub.status.busy": "2025-12-18T14:09:44.004253Z",
     "iopub.status.idle": "2025-12-18T14:09:44.014412Z",
     "shell.execute_reply": "2025-12-18T14:09:44.013416Z"
    }
   },
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def send_email(body: str):\n",
    "    \"\"\" Send out an email with the given body to all sales prospects \"\"\"\n",
    "    print(f\"--- TOOL CALLED: Sending Email via Brevo ---\")\n",
    "    api_instance = get_brevo_api_instance()\n",
    "    send_smtp_email = sib_api_v3_sdk.SendSmtpEmail(\n",
    "        to=RECIPIENT_EMAIL,\n",
    "        sender=SENDER_EMAIL,\n",
    "        subject=\"Sales email\",\n",
    "        text_content=body\n",
    "    )\n",
    "    try:\n",
    "        api_instance.send_transac_email(send_smtp_email)\n",
    "        return {\"status\": \"success\"}\n",
    "    except ApiException as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c13efede",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool1 = sales_agent1.as_tool(\n",
    "    tool_name=\"sales_agent1\",\n",
    "    tool_description=\"Write a cold sales email\"\n",
    ")\n",
    "\n",
    "tool2 = sales_agent2.as_tool(\n",
    "    tool_name=\"sales_agent2\",\n",
    "    tool_description=\"Write a cold sales email\"\n",
    ")\n",
    "\n",
    "tool3 = sales_agent3.as_tool(\n",
    "    tool_name=\"sales_agent3\",\n",
    "    tool_description=\"Write a cold sales email\"\n",
    ")\n",
    "\n",
    "tools = [tool1, tool2, tool3, send_email]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a77b739",
   "metadata": {},
   "source": [
    "### Sales Manager Agent\n",
    "\n",
    "The Sales Manager coordinates the drafting and sending process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7078f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T14:09:44.017368Z",
     "iopub.status.busy": "2025-12-18T14:09:44.017368Z",
     "iopub.status.idle": "2025-12-18T14:09:45.348214Z",
     "shell.execute_reply": "2025-12-18T14:09:45.347149Z"
    }
   },
   "outputs": [],
   "source": [
    "manager_instructions = \"\"\"\n",
    "You are a Sales Manager.\n",
    "1. Use sales_agent1, sales_agent2, and sales_agent3 to generate 3 drafts.\n",
    "2. Pick the best one.\n",
    "3. Use send_email to send ONLY the best one.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30e10220",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_manager_with_fallback(message):\n",
    "    last_error = None\n",
    "\n",
    "    for model in FREE_MODELS:\n",
    "        print(f\"üîÅ Sales Manager ‚Üí Trying model: {model}\")\n",
    "\n",
    "        sales_manager = Agent(\n",
    "            name=\"Sales Manager\",\n",
    "            instructions=manager_instructions,\n",
    "            tools=tools,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = await Runner.run(sales_manager, message)\n",
    "            print(f\"‚úÖ Sales Manager succeeded with {model}\\n\")\n",
    "            return result.final_output\n",
    "\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"‚ö†Ô∏è Sales Manager failed on {model}\")\n",
    "            print(f\"Reason: {e}\\n\")\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    print(\"‚ùå Sales Manager: all free models failed.\")\n",
    "    raise last_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cb6930f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Running Sales Manager (Tool Use) ---\n",
      "üîÅ Sales Manager ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Sales Manager failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}\n",
      "\n",
      "üîÅ Sales Manager ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Sales Manager failed on litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "Reason: litellm.Timeout: Timeout Error: OpenrouterException - litellm.Timeout: Connection timed out. Timeout passed=600.0, time taken=600.783 seconds\n",
      "\n",
      "üîÅ Sales Manager ‚Üí Trying model: litellm/openrouter/tngtech/deepseek-r1t2-chimera:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Sales Manager failed on litellm/openrouter/tngtech/deepseek-r1t2-chimera:free\n",
      "Reason: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}\n",
      "\n",
      "üîÅ Sales Manager ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.3-70b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Sales Manager failed on litellm/openrouter/meta-llama/llama-3.3-70b-instruct:free\n",
      "Reason: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"Tools are not supported in streaming mode.\\\"}\",\"provider_name\":\"ModelRun\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "üîÅ Sales Manager ‚Üí Trying model: litellm/openrouter/nousresearch/hermes-3-llama-3.1-405b:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Sales Manager failed on litellm/openrouter/nousresearch/hermes-3-llama-3.1-405b:free\n",
      "Reason: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}\n",
      "\n",
      "‚ùå Sales Manager: all free models failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x00000217DBFEE600>\n",
      "C:\\Users\\Y\\AppData\\Roaming\\uv\\python\\cpython-3.12.12-windows-x86_64-none\\Lib\\ast.py:280: RuntimeWarning: coroutine 'run_agents' was never awaited\n",
      "  yield item\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:157\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_async_call\u001b[39m\u001b[34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m async_httpx_client.post(\n\u001b[32m    158\u001b[39m         url=api_base,\n\u001b[32m    159\u001b[39m         headers=headers,\n\u001b[32m    160\u001b[39m         data=(\n\u001b[32m    161\u001b[39m             signed_json_body\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m signed_json_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    163\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m json.dumps(data)\n\u001b[32m    164\u001b[39m         ),\n\u001b[32m    165\u001b[39m         timeout=timeout,\n\u001b[32m    166\u001b[39m         stream=stream,\n\u001b[32m    167\u001b[39m         logging_obj=logging_obj,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://openrouter.ai/api/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenRouterException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\main.py:607\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:289\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.async_completion\u001b[39m\u001b[34m(self, custom_llm_provider, provider_config, api_base, headers, data, timeout, model, model_response, logging_obj, messages, optional_params, litellm_params, encoding, api_key, client, json_mode, signed_json_body, shared_session)\u001b[39m\n\u001b[32m    287\u001b[39m     async_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_common_async_call(\n\u001b[32m    290\u001b[39m     async_httpx_client=async_httpx_client,\n\u001b[32m    291\u001b[39m     provider_config=provider_config,\n\u001b[32m    292\u001b[39m     api_base=api_base,\n\u001b[32m    293\u001b[39m     headers=headers,\n\u001b[32m    294\u001b[39m     data=data,\n\u001b[32m    295\u001b[39m     timeout=timeout,\n\u001b[32m    296\u001b[39m     litellm_params=litellm_params,\n\u001b[32m    297\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    298\u001b[39m     logging_obj=logging_obj,\n\u001b[32m    299\u001b[39m     signed_json_body=signed_json_body,\n\u001b[32m    300\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    302\u001b[39m     model=model,\n\u001b[32m    303\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     json_mode=json_mode,\n\u001b[32m    313\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:182\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_async_call\u001b[39m\u001b[34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:3617\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   3611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   3612\u001b[39m         status_code=status_code,\n\u001b[32m   3613\u001b[39m         message=error_text,\n\u001b[32m   3614\u001b[39m         headers=error_headers,\n\u001b[32m   3615\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3617\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   3618\u001b[39m     error_message=error_text,\n\u001b[32m   3619\u001b[39m     status_code=status_code,\n\u001b[32m   3620\u001b[39m     headers=error_headers,\n\u001b[32m   3621\u001b[39m )\n",
      "\u001b[31mOpenRouterException\u001b[39m: {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 3. Running Sales Manager (Tool Use) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_manager_with_fallback(\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSend a cold sales email to the CEO\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mrun_manager_with_fallback\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m0.5\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå Sales Manager: all free models failed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m last_error\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrun_manager_with_fallback\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m      7\u001b[39m sales_manager = Agent(\n\u001b[32m      8\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mSales Manager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     instructions=manager_instructions,\n\u001b[32m     10\u001b[39m     tools=tools,\n\u001b[32m     11\u001b[39m     model=model,\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(sales_manager, message)\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Sales Manager succeeded with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.final_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:367\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    320\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    368\u001b[39m     starting_agent,\n\u001b[32m    369\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    370\u001b[39m     context=context,\n\u001b[32m    371\u001b[39m     max_turns=max_turns,\n\u001b[32m    372\u001b[39m     hooks=hooks,\n\u001b[32m    373\u001b[39m     run_config=run_config,\n\u001b[32m    374\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    375\u001b[39m     auto_previous_response_id=auto_previous_response_id,\n\u001b[32m    376\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    377\u001b[39m     session=session,\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:661\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    653\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    654\u001b[39m         starting_agent,\n\u001b[32m    655\u001b[39m         sequential_guardrails,\n\u001b[32m    656\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    657\u001b[39m         context_wrapper,\n\u001b[32m    658\u001b[39m     )\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    662\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    663\u001b[39m         starting_agent,\n\u001b[32m    664\u001b[39m         parallel_guardrails,\n\u001b[32m    665\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    666\u001b[39m         context_wrapper,\n\u001b[32m    667\u001b[39m     ),\n\u001b[32m    668\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    669\u001b[39m         agent=current_agent,\n\u001b[32m    670\u001b[39m         all_tools=all_tools,\n\u001b[32m    671\u001b[39m         original_input=original_input,\n\u001b[32m    672\u001b[39m         generated_items=generated_items,\n\u001b[32m    673\u001b[39m         hooks=hooks,\n\u001b[32m    674\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    675\u001b[39m         run_config=run_config,\n\u001b[32m    676\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    677\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    678\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    679\u001b[39m     ),\n\u001b[32m    680\u001b[39m )\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    683\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1602\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1600\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1602\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1603\u001b[39m     agent,\n\u001b[32m   1604\u001b[39m     system_prompt,\n\u001b[32m   1605\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1606\u001b[39m     output_schema,\n\u001b[32m   1607\u001b[39m     all_tools,\n\u001b[32m   1608\u001b[39m     handoffs,\n\u001b[32m   1609\u001b[39m     hooks,\n\u001b[32m   1610\u001b[39m     context_wrapper,\n\u001b[32m   1611\u001b[39m     run_config,\n\u001b[32m   1612\u001b[39m     tool_use_tracker,\n\u001b[32m   1613\u001b[39m     server_conversation_tracker,\n\u001b[32m   1614\u001b[39m     prompt_config,\n\u001b[32m   1615\u001b[39m )\n\u001b[32m   1617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1618\u001b[39m     agent=agent,\n\u001b[32m   1619\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1628\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1629\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1860\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1850\u001b[39m previous_response_id = (\n\u001b[32m   1851\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1853\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m server_conversation_tracker.previous_response_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1855\u001b[39m )\n\u001b[32m   1856\u001b[39m conversation_id = (\n\u001b[32m   1857\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1858\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1860\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1861\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1863\u001b[39m     model_settings=model_settings,\n\u001b[32m   1864\u001b[39m     tools=all_tools,\n\u001b[32m   1865\u001b[39m     output_schema=output_schema,\n\u001b[32m   1866\u001b[39m     handoffs=handoffs,\n\u001b[32m   1867\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1868\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1869\u001b[39m     ),\n\u001b[32m   1870\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1871\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1872\u001b[39m     prompt=prompt_config,\n\u001b[32m   1873\u001b[39m )\n\u001b[32m   1875\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1877\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\extensions\\models\\litellm_model.py:100\u001b[39m, in \u001b[36mLitellmModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     83\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     prompt: Any | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     93\u001b[39m ) -> ModelResponse:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     95\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     96\u001b[39m         model_config=model_settings.to_json_dict()\n\u001b[32m     97\u001b[39m         | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mmodel_impl\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     98\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     99\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    101\u001b[39m             system_instructions,\n\u001b[32m    102\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    103\u001b[39m             model_settings,\n\u001b[32m    104\u001b[39m             tools,\n\u001b[32m    105\u001b[39m             output_schema,\n\u001b[32m    106\u001b[39m             handoffs,\n\u001b[32m    107\u001b[39m             span_generation,\n\u001b[32m    108\u001b[39m             tracing,\n\u001b[32m    109\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    110\u001b[39m             prompt=prompt,\n\u001b[32m    111\u001b[39m         )\n\u001b[32m    113\u001b[39m         message: litellm.types.utils.Message | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    114\u001b[39m         first_choice: litellm.types.utils.Choices | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\extensions\\models\\litellm_model.py:393\u001b[39m, in \u001b[36mLitellmModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;66;03m# Prevent duplicate reasoning_effort kwargs when it was promoted to a top-level argument.\u001b[39;00m\n\u001b[32m    391\u001b[39m extra_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m litellm.acompletion(\n\u001b[32m    394\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    395\u001b[39m     messages=converted_messages,\n\u001b[32m    396\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    397\u001b[39m     temperature=model_settings.temperature,\n\u001b[32m    398\u001b[39m     top_p=model_settings.top_p,\n\u001b[32m    399\u001b[39m     frequency_penalty=model_settings.frequency_penalty,\n\u001b[32m    400\u001b[39m     presence_penalty=model_settings.presence_penalty,\n\u001b[32m    401\u001b[39m     max_tokens=model_settings.max_tokens,\n\u001b[32m    402\u001b[39m     tool_choice=\u001b[38;5;28mself\u001b[39m._remove_not_given(tool_choice),\n\u001b[32m    403\u001b[39m     response_format=\u001b[38;5;28mself\u001b[39m._remove_not_given(response_format),\n\u001b[32m    404\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    405\u001b[39m     stream=stream,\n\u001b[32m    406\u001b[39m     stream_options=stream_options,\n\u001b[32m    407\u001b[39m     reasoning_effort=reasoning_effort,\n\u001b[32m    408\u001b[39m     top_logprobs=model_settings.top_logprobs,\n\u001b[32m    409\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    410\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.api_key,\n\u001b[32m    411\u001b[39m     base_url=\u001b[38;5;28mself\u001b[39m.base_url,\n\u001b[32m    412\u001b[39m     **extra_kwargs,\n\u001b[32m    413\u001b[39m )\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, litellm.types.utils.ModelResponse):\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\utils.py:1646\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1644\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1645\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\utils.py:1492\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1489\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1491\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1493\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1495\u001b[39m     kwargs=kwargs,\n\u001b[32m   1496\u001b[39m     call_type=call_type,\n\u001b[32m   1497\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\main.py:626\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    625\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2340\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2339\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2342\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2220\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m   2219\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2220\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[32m   2221\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNotFoundError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2222\u001b[39m         model=model,\n\u001b[32m   2223\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   2224\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   2225\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   2226\u001b[39m     )\n\u001b[32m   2227\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m408\u001b[39m:\n\u001b[32m   2228\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 3. Running Sales Manager (Tool Use) ---\")\n",
    "\n",
    "await run_manager_with_fallback(\n",
    "    \"Send a cold sales email to the CEO\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fe69f",
   "metadata": {},
   "source": [
    "## Step 3: Agent Handoffs\n",
    "\n",
    "For more complex workflows, we use **Handoffs**. Here, we'll create a specialized `Email Manager` that handles formatting (HTML) and Subject lines before sending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbae6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Specialist Agents\n",
    "subject_writer = Agent(\n",
    "    name=\"Subject Writer\",\n",
    "    instructions=\"Write a catchy subject line.\"\n",
    ")\n",
    "\n",
    "subject_tool = subject_writer.as_tool(\n",
    "    tool_name=\"subject_writer\",\n",
    "    tool_description=\"Write a subject\"\n",
    ")\n",
    "\n",
    "html_converter = Agent(\n",
    "    name=\"HTML Converter\",\n",
    "    instructions=\"Convert text to HTML.\"\n",
    ")\n",
    "\n",
    "html_tool = html_converter.as_tool(\n",
    "    tool_name=\"html_converter\",\n",
    "    tool_description=\"Convert to HTML\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36517691",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def send_html_email(subject: str, html_body: str):\n",
    "    \"\"\" Send HTML email \"\"\"\n",
    "    print(f\"--- TOOL CALLED: Sending HTML Email via Brevo ---\")\n",
    "    api_instance = get_brevo_api_instance()\n",
    "    send_smtp_email = sib_api_v3_sdk.SendSmtpEmail(\n",
    "        to=RECIPIENT_EMAIL,\n",
    "        sender=SENDER_EMAIL,\n",
    "        subject=subject,\n",
    "        html_content=html_body\n",
    "    )\n",
    "    try:\n",
    "        api_instance.send_transac_email(send_smtp_email)\n",
    "        return {\"status\": \"success\"}\n",
    "    except ApiException as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1c83a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "emailer_agent_instructions = \"Use tools to write subject, convert to HTML, then send.\"\n",
    "\n",
    "# Tools: subject, html, send\n",
    "tools_for_emailer = [subject_tool, html_tool, send_html_email]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba96f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_manager_instructions = (\n",
    "    \"Generate 3 drafts using tools, pick the best, then handoff to Email Manager.\"\n",
    ")\n",
    "\n",
    "# Drafting tools: sales agents\n",
    "drafting_tools = [tool1, tool2, tool3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5a479c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_manager_fallback(agent_name, instructions, message, tools=None, handoffs=None):\n",
    "    last_error = None\n",
    "\n",
    "    for model in FREE_MODELS:\n",
    "        print(f\"üîÅ {agent_name} ‚Üí Trying model: {model}\")\n",
    "\n",
    "        agent = Agent(\n",
    "            name=agent_name,\n",
    "            instructions=instructions,\n",
    "            tools=tools,\n",
    "            handoffs=handoffs,\n",
    "            model=model,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = await Runner.run(agent, message)\n",
    "            print(f\"‚úÖ {agent_name} succeeded with {model}\\n\")\n",
    "            return result.final_output\n",
    "\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"‚ö†Ô∏è {agent_name} failed on {model}\")\n",
    "            print(f\"Reason: {e}\\n\")\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    print(f\"‚ùå {agent_name}: all free models failed.\")\n",
    "    raise last_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "607d7b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Running Final Handoff ---\n",
      "üîÅ Final Manager ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Final Manager failed on litellm/openrouter/meta-llama/llama-3.2-3b-instruct:free\n",
      "Reason: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}\n",
      "\n",
      "üîÅ Final Manager ‚Üí Trying model: litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Final Manager failed on litellm/openrouter/mistralai/mistral-7b-instruct:free\n",
      "Reason: litellm.Timeout: Timeout Error: OpenrouterException - litellm.Timeout: Connection timed out. Timeout passed=600.0, time taken=600.782 seconds\n",
      "\n",
      "üîÅ Final Manager ‚Üí Trying model: litellm/openrouter/tngtech/deepseek-r1t2-chimera:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Final Manager failed on litellm/openrouter/tngtech/deepseek-r1t2-chimera:free\n",
      "Reason: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}\n",
      "\n",
      "üîÅ Final Manager ‚Üí Trying model: litellm/openrouter/meta-llama/llama-3.3-70b-instruct:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Final Manager failed on litellm/openrouter/meta-llama/llama-3.3-70b-instruct:free\n",
      "Reason: litellm.BadRequestError: OpenrouterException - {\"error\":{\"message\":\"Provider returned error\",\"code\":400,\"metadata\":{\"raw\":\"{\\\"detail\\\":\\\"Tools are not supported in streaming mode.\\\"}\",\"provider_name\":\"ModelRun\"}},\"user_id\":\"user_35bH96ZYgxpOm3JCUhaUt3Iq1s8\"}\n",
      "\n",
      "üîÅ Final Manager ‚Üí Trying model: litellm/openrouter/nousresearch/hermes-3-llama-3.1-405b:free\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "‚ö†Ô∏è Final Manager failed on litellm/openrouter/nousresearch/hermes-3-llama-3.1-405b:free\n",
      "Reason: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}\n",
      "\n",
      "‚ùå Final Manager: all free models failed.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:157\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_async_call\u001b[39m\u001b[34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m async_httpx_client.post(\n\u001b[32m    158\u001b[39m         url=api_base,\n\u001b[32m    159\u001b[39m         headers=headers,\n\u001b[32m    160\u001b[39m         data=(\n\u001b[32m    161\u001b[39m             signed_json_body\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m signed_json_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    163\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m json.dumps(data)\n\u001b[32m    164\u001b[39m         ),\n\u001b[32m    165\u001b[39m         timeout=timeout,\n\u001b[32m    166\u001b[39m         stream=stream,\n\u001b[32m    167\u001b[39m         logging_obj=logging_obj,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '404 Not Found' for url 'https://openrouter.ai/api/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenRouterException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\main.py:607\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:289\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.async_completion\u001b[39m\u001b[34m(self, custom_llm_provider, provider_config, api_base, headers, data, timeout, model, model_response, logging_obj, messages, optional_params, litellm_params, encoding, api_key, client, json_mode, signed_json_body, shared_session)\u001b[39m\n\u001b[32m    287\u001b[39m     async_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_common_async_call(\n\u001b[32m    290\u001b[39m     async_httpx_client=async_httpx_client,\n\u001b[32m    291\u001b[39m     provider_config=provider_config,\n\u001b[32m    292\u001b[39m     api_base=api_base,\n\u001b[32m    293\u001b[39m     headers=headers,\n\u001b[32m    294\u001b[39m     data=data,\n\u001b[32m    295\u001b[39m     timeout=timeout,\n\u001b[32m    296\u001b[39m     litellm_params=litellm_params,\n\u001b[32m    297\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    298\u001b[39m     logging_obj=logging_obj,\n\u001b[32m    299\u001b[39m     signed_json_body=signed_json_body,\n\u001b[32m    300\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    302\u001b[39m     model=model,\n\u001b[32m    303\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     json_mode=json_mode,\n\u001b[32m    313\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:182\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_async_call\u001b[39m\u001b[34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\llms\\custom_httpx\\llm_http_handler.py:3617\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   3611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   3612\u001b[39m         status_code=status_code,\n\u001b[32m   3613\u001b[39m         message=error_text,\n\u001b[32m   3614\u001b[39m         headers=error_headers,\n\u001b[32m   3615\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3617\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   3618\u001b[39m     error_message=error_text,\n\u001b[32m   3619\u001b[39m     status_code=status_code,\n\u001b[32m   3620\u001b[39m     headers=error_headers,\n\u001b[32m   3621\u001b[39m )\n",
      "\u001b[31mOpenRouterException\u001b[39m: {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 4. Running Final Handoff ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m final_output = \u001b[38;5;28;01mawait\u001b[39;00m run_manager_fallback(\n\u001b[32m      4\u001b[39m     agent_name=\u001b[33m\"\u001b[39m\u001b[33mFinal Manager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     instructions=final_manager_instructions,\n\u001b[32m      6\u001b[39m     message=\u001b[33m\"\u001b[39m\u001b[33mSend a cold sales email to the CEO from Alice\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     tools=drafting_tools,\n\u001b[32m      8\u001b[39m     handoffs=[\n\u001b[32m      9\u001b[39m         Agent(\n\u001b[32m     10\u001b[39m             name=\u001b[33m\"\u001b[39m\u001b[33mEmail Manager\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m             instructions=emailer_agent_instructions,\n\u001b[32m     12\u001b[39m             tools=tools_for_emailer,\n\u001b[32m     13\u001b[39m             handoff_description=\u001b[33m\"\u001b[39m\u001b[33mFormat and send email\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m         )\n\u001b[32m     15\u001b[39m     ]\n\u001b[32m     16\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, final_output)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_manager_fallback\u001b[39m\u001b[34m(agent_name, instructions, message, tools, handoffs)\u001b[39m\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m0.5\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: all free models failed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m last_error\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mrun_manager_fallback\u001b[39m\u001b[34m(agent_name, instructions, message, tools, handoffs)\u001b[39m\n\u001b[32m      7\u001b[39m agent = Agent(\n\u001b[32m      8\u001b[39m     name=agent_name,\n\u001b[32m      9\u001b[39m     instructions=instructions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     model=model,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, message)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m succeeded with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.final_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:367\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, auto_previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    320\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    366\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    368\u001b[39m     starting_agent,\n\u001b[32m    369\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    370\u001b[39m     context=context,\n\u001b[32m    371\u001b[39m     max_turns=max_turns,\n\u001b[32m    372\u001b[39m     hooks=hooks,\n\u001b[32m    373\u001b[39m     run_config=run_config,\n\u001b[32m    374\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    375\u001b[39m     auto_previous_response_id=auto_previous_response_id,\n\u001b[32m    376\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    377\u001b[39m     session=session,\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:661\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    653\u001b[39m     sequential_results = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    654\u001b[39m         starting_agent,\n\u001b[32m    655\u001b[39m         sequential_guardrails,\n\u001b[32m    656\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    657\u001b[39m         context_wrapper,\n\u001b[32m    658\u001b[39m     )\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# Run parallel guardrails + agent together.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    662\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    663\u001b[39m         starting_agent,\n\u001b[32m    664\u001b[39m         parallel_guardrails,\n\u001b[32m    665\u001b[39m         _copy_str_or_list(prepared_input),\n\u001b[32m    666\u001b[39m         context_wrapper,\n\u001b[32m    667\u001b[39m     ),\n\u001b[32m    668\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    669\u001b[39m         agent=current_agent,\n\u001b[32m    670\u001b[39m         all_tools=all_tools,\n\u001b[32m    671\u001b[39m         original_input=original_input,\n\u001b[32m    672\u001b[39m         generated_items=generated_items,\n\u001b[32m    673\u001b[39m         hooks=hooks,\n\u001b[32m    674\u001b[39m         context_wrapper=context_wrapper,\n\u001b[32m    675\u001b[39m         run_config=run_config,\n\u001b[32m    676\u001b[39m         should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    677\u001b[39m         tool_use_tracker=tool_use_tracker,\n\u001b[32m    678\u001b[39m         server_conversation_tracker=server_conversation_tracker,\n\u001b[32m    679\u001b[39m     ),\n\u001b[32m    680\u001b[39m )\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# Combine sequential and parallel results.\u001b[39;00m\n\u001b[32m    683\u001b[39m input_guardrail_results = sequential_results + input_guardrail_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1602\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, server_conversation_tracker)\u001b[39m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1600\u001b[39m     \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1602\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1603\u001b[39m     agent,\n\u001b[32m   1604\u001b[39m     system_prompt,\n\u001b[32m   1605\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1606\u001b[39m     output_schema,\n\u001b[32m   1607\u001b[39m     all_tools,\n\u001b[32m   1608\u001b[39m     handoffs,\n\u001b[32m   1609\u001b[39m     hooks,\n\u001b[32m   1610\u001b[39m     context_wrapper,\n\u001b[32m   1611\u001b[39m     run_config,\n\u001b[32m   1612\u001b[39m     tool_use_tracker,\n\u001b[32m   1613\u001b[39m     server_conversation_tracker,\n\u001b[32m   1614\u001b[39m     prompt_config,\n\u001b[32m   1615\u001b[39m )\n\u001b[32m   1617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1618\u001b[39m     agent=agent,\n\u001b[32m   1619\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1628\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1629\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1860\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, server_conversation_tracker, prompt_config)\u001b[39m\n\u001b[32m   1850\u001b[39m previous_response_id = (\n\u001b[32m   1851\u001b[39m     server_conversation_tracker.previous_response_id\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker\n\u001b[32m   1853\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m server_conversation_tracker.previous_response_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1855\u001b[39m )\n\u001b[32m   1856\u001b[39m conversation_id = (\n\u001b[32m   1857\u001b[39m     server_conversation_tracker.conversation_id \u001b[38;5;28;01mif\u001b[39;00m server_conversation_tracker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1858\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1860\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1861\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1862\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1863\u001b[39m     model_settings=model_settings,\n\u001b[32m   1864\u001b[39m     tools=all_tools,\n\u001b[32m   1865\u001b[39m     output_schema=output_schema,\n\u001b[32m   1866\u001b[39m     handoffs=handoffs,\n\u001b[32m   1867\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1868\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1869\u001b[39m     ),\n\u001b[32m   1870\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1871\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1872\u001b[39m     prompt=prompt_config,\n\u001b[32m   1873\u001b[39m )\n\u001b[32m   1875\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1877\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\extensions\\models\\litellm_model.py:100\u001b[39m, in \u001b[36mLitellmModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     83\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     prompt: Any | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     93\u001b[39m ) -> ModelResponse:\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     95\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     96\u001b[39m         model_config=model_settings.to_json_dict()\n\u001b[32m     97\u001b[39m         | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mmodel_impl\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlitellm\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     98\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     99\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    101\u001b[39m             system_instructions,\n\u001b[32m    102\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    103\u001b[39m             model_settings,\n\u001b[32m    104\u001b[39m             tools,\n\u001b[32m    105\u001b[39m             output_schema,\n\u001b[32m    106\u001b[39m             handoffs,\n\u001b[32m    107\u001b[39m             span_generation,\n\u001b[32m    108\u001b[39m             tracing,\n\u001b[32m    109\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    110\u001b[39m             prompt=prompt,\n\u001b[32m    111\u001b[39m         )\n\u001b[32m    113\u001b[39m         message: litellm.types.utils.Message | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    114\u001b[39m         first_choice: litellm.types.utils.Choices | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\agents\\extensions\\models\\litellm_model.py:393\u001b[39m, in \u001b[36mLitellmModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;66;03m# Prevent duplicate reasoning_effort kwargs when it was promoted to a top-level argument.\u001b[39;00m\n\u001b[32m    391\u001b[39m extra_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m litellm.acompletion(\n\u001b[32m    394\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    395\u001b[39m     messages=converted_messages,\n\u001b[32m    396\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    397\u001b[39m     temperature=model_settings.temperature,\n\u001b[32m    398\u001b[39m     top_p=model_settings.top_p,\n\u001b[32m    399\u001b[39m     frequency_penalty=model_settings.frequency_penalty,\n\u001b[32m    400\u001b[39m     presence_penalty=model_settings.presence_penalty,\n\u001b[32m    401\u001b[39m     max_tokens=model_settings.max_tokens,\n\u001b[32m    402\u001b[39m     tool_choice=\u001b[38;5;28mself\u001b[39m._remove_not_given(tool_choice),\n\u001b[32m    403\u001b[39m     response_format=\u001b[38;5;28mself\u001b[39m._remove_not_given(response_format),\n\u001b[32m    404\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    405\u001b[39m     stream=stream,\n\u001b[32m    406\u001b[39m     stream_options=stream_options,\n\u001b[32m    407\u001b[39m     reasoning_effort=reasoning_effort,\n\u001b[32m    408\u001b[39m     top_logprobs=model_settings.top_logprobs,\n\u001b[32m    409\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    410\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.api_key,\n\u001b[32m    411\u001b[39m     base_url=\u001b[38;5;28mself\u001b[39m.base_url,\n\u001b[32m    412\u001b[39m     **extra_kwargs,\n\u001b[32m    413\u001b[39m )\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, litellm.types.utils.ModelResponse):\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\utils.py:1646\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1644\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1645\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\utils.py:1492\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1489\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1491\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1493\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1495\u001b[39m     kwargs=kwargs,\n\u001b[32m   1496\u001b[39m     call_type=call_type,\n\u001b[32m   1497\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\main.py:626\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    625\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2340\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2339\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2342\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Y\\Desktop\\Agentic AI Projects\\OPENAI AGENT SDK\\openai-agent-sdk\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2220\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m   2219\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2220\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFoundError(\n\u001b[32m   2221\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNotFoundError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2222\u001b[39m         model=model,\n\u001b[32m   2223\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m   2224\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   2225\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m   2226\u001b[39m     )\n\u001b[32m   2227\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m408\u001b[39m:\n\u001b[32m   2228\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mNotFoundError\u001b[39m: litellm.NotFoundError: NotFoundError: OpenrouterException - {\"error\":{\"message\":\"No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/guides/routing/provider-selection\",\"code\":404}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 4. Running Final Handoff ---\")\n",
    "\n",
    "final_output = await run_manager_fallback(\n",
    "    agent_name=\"Final Manager\",\n",
    "    instructions=final_manager_instructions,\n",
    "    message=\"Send a cold sales email to the CEO from Alice\",\n",
    "    tools=drafting_tools,\n",
    "    handoffs=[\n",
    "        Agent(\n",
    "            name=\"Email Manager\",\n",
    "            instructions=emailer_agent_instructions,\n",
    "            tools=tools_for_emailer,\n",
    "            handoff_description=\"Format and send email\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nFinal output:\\n\", final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0f7d5e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have now built a sophisticated multi-agent system that:\n",
    "1.  Uses parallel processing to generate diversity.\n",
    "2.  Uses selection logic to pick quality.\n",
    "3.  Uses specialized tools to interact with real-world APIs (Brevo).\n",
    "4.  Uses handoffs to delegate specialized tasks (formatting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-agent-sdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
